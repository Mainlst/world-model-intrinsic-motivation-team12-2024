---
marp: true
theme: academic
paginate: true
math: katex
style: |
  .cjk_fallback {
    font-family: 'Noto Sans JP';
    font-size: 0.8em
  }
---
<!-- _header:  -->

## How to Stay Curious while avoiding Noisy TVs using Aleatoric Uncertainty Estimation

[arXiv:2102.04399v3](https://arxiv.org/html/2102.04399v3)

---
<!-- _header: Abstract  -->

<div style="font-size: 0.6em">

#### 研究背景
- 強化学習エージェントは、外的報酬が乏しい環境で探索が困難
- 内的動機としての「好奇心」に基づいた方法は、探索性能を向上させる可能性があるが、行動依存のノイズ（Noisy TV Probrem）に直面すると探索が妨げられるという問題がある。
#### 研究目的
- Noisy TV Probremを回避しつつ、探索効率を向上させる新しい好奇心駆動型エージェントの設計
#### 提案手法
- Aleatoric Mapping Agents (AMAs)と呼ばれる新しいアプローチを提案
  - 次の状態に関する平均とアレアトリック（偶然性）不確実性を独立して予測し、不確実性が高い状態遷移に対して内的報酬を低減する
- ノイズの多い状態遷移からエージェントが「好奇心をそがれる」ことを防ぐことが可能である
#### 結果
- AMAsは、従来の好奇心駆動型手法がNoisy TV Probremによって動きが制限される環境においても、その影響を回避することができた
- ランダムネットワーク蒸留やモデルアンサンブルといった既存手法が、これまでの想定より以上にノイズに脆弱であることを実証した
- AtariのBank Heistなどの自然発生的なノイズを含む環境でも、AMAsの有効性が確認された


</div>

---
<!-- _header: Background  -->

<div style="font-size: 0.6em">


**好奇心駆動型学習とその限界**
好奇心駆動型学習では、エージェントは自己監督的な予測モデルを用い、予測誤差を内的報酬として活用する
この方法により、外的報酬が乏しい環境でも、エージェントは未探索の状態空間に到達する動機付けを得る。
しかし、好奇心駆動型手法は、ノイジーな状態遷移に弱いという問題がある。
たとえば、ランダムなノイズによる状態変化にエージェントが興味を持ちすぎて、探索が停滞する


</div>


---
<!-- _header: Aleatoric Uncertainty  -->


<div style="font-size: 0.8em">

#### Epistemic Uncertainty
- システムやモデルについての知識の不足から生じる不確実性

#### Aleatoric Uncertainty  
システムやデータ自体に存在する偶然性や変動性から生じる不確実性

| **特性**           | **Epistemic Uncertainty**              | **Aleatoric Uncertainty**              |
|--------------------|--------------------------------|--------------------------------|
| **起因**          | 知識やモデルの不足                   | データの内在的なランダム性                  |
| **軽減の可能性**   | トレーニングデータやモデルの改善で軽減可能 | 完全には軽減できない                       |
| **例**            | 未知のクラスや外れ値の予測問題            | 測定誤差やノイズが支配的な予測問題            |

> ChatGPTの検索結果をReferenceフォルダにいれた


---
<!-- _header: 既存手法 -->
<div style="font-size: 0.7em">

#### 動的依存の予測誤差を削減する方法（Pathakら、2017年）
- Inverse Dynamics Feature (IDF) Curiosity
  - 逆動力学モデル
    - エージェントの「現在の状態と次の状態」から「取られた行動」を予測するを導入
    - 次の状態を引き起こすためにエージェントが行った行動を特定することを目的
  - 逆動力学モデルを通じて抽出された特徴空間で予測誤差
  → エージェントが直接的に制御可能な要素が重要視される
  → Noisy TVはエージェントの行動に依存しないため、制御可能な要素とならず、エージェントが執着しない

####  複数のモデルを利用したアンサンブル手法（Shyamら、2019年）
- アンサンブルモデルの活用
  - 複数の予測モデル（アンサンブル）を同時に訓練し、それぞれのモデルが異なる予測を行うことで、
  予測のばらつき（不確実性）を計測する。
  - アンサンブル内での予測のばらつきが大きい場合、それは環境が未知であるか、
  学習可能な状態であることを示す。

</div>

---
<!-- _header: 既存手法 -->
<div style="font-size: 0.7em">

#### ダイナミクスを排除した探索技術（Burdaら、2019年）
- ランダムに初期化されたターゲットネットワークと、それを模倣しようとする学習可能な予測ネットワークの2つのネットワーク
  - ターゲットネットワークは一切訓練されないため、出力は固定されている。
  - 予測ネットワークは、与えられた状態に対してターゲットネットワークの出力を予測するように学習する
- 内的報酬 → 予測ネットワークの出力とターゲットネットワークの出力との間の誤差
  - 状態が「新しい」場合、予測ネットワークはターゲットネットワークの出力を正確に予測できず、誤差が大きくなる。
  - 状態が「既知」の場合、予測ネットワークはその状態を十分に学習しており、誤差が小さくなる。
- 状態遷移の予測（ダイナミクス）を全く利用しないため、ノイズ依存の状態遷移の影響を受けにくい。
  - 新奇性のみに依存
  RNDは、エージェントが新しい状態を発見する動機を提供するが、状態が新しいだけで価値がない場合（例: 無意味なノイズ）にも反応する可能性がある。
  - 報酬の収束問題
  状態空間全体が探索され、予測ネットワークが十分に訓練されると、内的報酬が減少して探索が停滞する可能性がある。
  - 報酬の設計が単一的
  外的報酬とのバランスが取れていない場合、学習が進まない可能性がある。

</div>

---
<!-- _header: 手法の説明 -->

<div style="font-size: 0.7em">

状態:$s \in \mathcal{S}$ 行動:$a \in \mathcal{A}$ 報酬:$r \in \mathcal{R} \subset \mathbb{R}$
Policy: $a_t \sim \pi(\cdot | s_t)$ 
状態遷移確率: $p(\mathbb{s}_{t+1},r_{t+1} |\mathbb{s},a_t )$ → 行動$a_t$の結果、報酬$r_{t+1}$を得て、状態$s_{t+1}$となる
Policy学習の目的関数：$\max_{\pi_\xi} \mathbb{E}_{\pi_\xi} \left[ \sum_{k=0}^{T} \gamma^k r_{t+k} \right]$ 　　エピソード長さ: $T$  割引率: $\gamma$
報酬： $r_i=\beta r_t^i + r_t^e$  Intrinsic: $r_t^i$ 　　Extrinsic（外発的）: $r_t^e$ $\beta$: ハイパーパラメータ

Intrinsic Reward(Kendall & Gal, 2017)
$r_t^i = \| \mathbf{s}_{t+1} - \hat{\mu}_{t+1} \|^2 - \eta \, \mathrm{Tr} \left( \hat{\Sigma}_{t+1} \right)$
$\hat{\mu}_{t+1}$状態予測の平均　　$\hat{\Sigma}_{t+1}$:状態予測の分散共分散行列(aleatoric uncertainty)

平均と分散の予測モデルは、 Double-Headed Neural Networkで行う(Kendall & Gal, 2017)

$p(\mathbf{s}_{1:N} \mid \theta, \phi) = \prod_{t=1}^N \mathcal{N}\left(\mathbf{s}_{t+1} ; \mathbf{f}_\theta(\mathbf{s}_t, \mathbf{a}_t), \mathbf{g}_\phi(\mathbf{s}_t, \mathbf{a}_t)\right)$ 
状態遷移はガウス分布に基づき、その平均$\mathbf{f}_\theta(\mathbf{s}_t, \mathbf{a}_t)$と分散$\mathbf{g}_\phi(\mathbf{s}_t, \mathbf{a}_t)$で決まる

> Double-Headed Neural Network : ChatGPTの検索結果をReferenceフォルダにいれた
---
<!-- _header: 手法の説明：推定方法-->

<div style="font-size: 0.7em">

Kendall & Gal (2017) の手法のアプローチ：MAP推定
MAP推定で使用されるゼロ平均ガウス事前分布に基づく正則化項が、モデルの性能向上に寄与しない
$\|\theta\|^2や\|\phi\|^2$を最小化するような正則化を加えても、モデルの予測性能や不確実性推定に有意な影響が見られない
→単純な最尤法でOK

$\mathcal{L}_{t+1}(\theta, \phi) = (\mathbf{s}_{t+1} - \hat{\mu}_{t+1})^\top \hat{\Sigma}_{t+1}^{-1} (\mathbf{s}_{t+1} - \hat{\mu}_{t+1}) + \lambda \log(\det(\hat{\Sigma}_{t+1}))$

第1項: 平均誤差を分散でスケーリング（不確実性の信頼度に応じて誤差を重み付け）
第2項: 分散の対数を正則化する項で、分散が過剰に大きくなるのを防ぐ
→ 事前分布に依存しないため、計算がシンプル

</div>

---
<!-- _header: 実験結果-->

<div style="font-size: 0.65em">

**1. Noisy MNIST**
- 環境には確定的遷移（0→0）と確率的遷移（1→ランダムな2-9）を含むMNIST画像ペアを用いた。
- AMAsは、確率的遷移を認識し、それに対する内部報酬を低減することで、ランダムなノイズに引き寄せられることを防いでいる。

**2. Minigrid環境**
- 部屋が複数ある環境（4部屋および6部屋）で探索をテストした。
- AMAsは、行動依存型のノイズ（例：ノイズTV）にほとんど影響を受けず、従来の手法よりも広範囲な探索を達成している。

**3. ビデオゲーム環境**
- 高次元のRetroビデオゲーム（Space Invaders, Mario）でテストを行った。
- AMAsは、行動によるノイズ生成に対しても頑健であり、従来の手法を凌駕する性能を示している。

**4. 自然発生するノイズ（Bank Heistゲーム）**
- Atariゲーム「Bank Heist」において自然に発生する確率的なトラップを調査した。
- AMAsは、このトラップを回避し、安定して環境を探索する能力を示している。

</div>

---
<!-- _header: 考察-->

<div style="font-size: 0.6em">

**1. 成果の意義**
- AMAsは、従来の好奇心駆動型エージェントが抱える「ノイズTV問題」を克服している。
- 高エントロピーな確率的環境においても効果的な探索を実現している。
- 提案手法は、人工知能エージェントにおける探索の質を向上させる可能性を示している。

**2. 限界**
- Aleatoric不確実性とEpistemic不確実性の分離は理論的には保証されているが、完全には達成できない場合がある。
- 特に未知の領域では、不確実性の推定が信頼できない可能性がある。

**3. 将来的な展望**
- AMAのアプローチを、ピクセル以外の特徴空間やダイナミクスを回避する方法（例：ランダムネットワーク蒸留）に統合することで、さらなる改善が期待できる。
- AMAの失敗ケースを分析し、確率的環境での従来手法の課題解決に役立てるべきである。

**成果と意義のまとめ**
提案手法であるAMAsは、行動依存型の確率的トラップを回避しながら、探索能力を維持または向上させることを実証している。これは、強化学習エージェントがノイズに引き寄せられるという従来の問題を解決するものであり、人工知能やロボティクス分野での応用可能性を広げる重要な成果である。
